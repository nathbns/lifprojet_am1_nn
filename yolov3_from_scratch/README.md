# YOLOv3 - You Only Look Once (Version 3)

Impl√©mentation from scratch de l'architecture YOLOv3 avec Darknet-53 et Feature Pyramid Network (FPN).

![Architecture YOLOv3](architecture_yolov3.png)

## Description

YOLOv3 repr√©sente une √©volution majeure de YOLO avec l'introduction de la d√©tection multi-√©chelle. Cette architecture am√©liore consid√©rablement la d√©tection des petits objets gr√¢ce √† son Feature Pyramid Network (FPN) qui pr√©dit √† trois √©chelles diff√©rentes.

**Innovations cl√©s par rapport √† YOLOv1:**
- **D√©tection multi-√©chelle:** 3 niveaux de pr√©diction (13√ó13, 26√ó26, 52√ó52)
- **Backbone am√©lior√©:** Darknet-53 avec blocs r√©siduels
- **Meilleure pr√©cision:** Surtout pour les petits objets
- **Feature Pyramid Network:** Connexions skip pour combiner features de diff√©rentes r√©solutions
- **Anchors pr√©d√©finis:** 9 anchors (3 par √©chelle)
- **Pas de fully connected:** Architecture enti√®rement convolutive

## Architecture

### 1. Backbone: Darknet-53

Le backbone Darknet-53 est compos√© de **53 couches convolutives** avec des blocs r√©siduels:

```
Input (416√ó416√ó3)
‚Üì
Conv (3√ó3√ó32)
‚Üì
Conv + Residual√ó1  (64 channels)   ‚îÄ‚îÄ‚îê
‚Üì                                    ‚îÇ
Conv + Residual√ó2  (128 channels)  ‚îÄ‚îÄ‚î§
‚Üì                                    ‚îÇ
Conv + Residual√ó8  (256 channels)  ‚îÄ‚îÄ‚î§‚îÄ‚Üí Route 1 (vers Scale 3)
‚Üì                                    ‚îÇ
Conv + Residual√ó8  (512 channels)  ‚îÄ‚îÄ‚î§‚îÄ‚Üí Route 2 (vers Scale 2)
‚Üì                                    ‚îÇ
Conv + Residual√ó4  (1024 channels) ‚îÄ‚îÄ‚îò
‚Üì
Neck (FPN)
```

**Caract√©ristiques:**
- **Residual Blocks:** Connexions r√©siduelles type ResNet pour stabilit√©
- **Batch Normalization:** Apr√®s chaque couche convolutive
- **LeakyReLU:** Activation (Œ±=0.1)
- **Pas de pooling:** Stride=2 pour downsampling
- **Param√®tres:** ~41 millions pour le backbone seul

### 2. Neck: Feature Pyramid Network (FPN)

Le FPN combine des features de diff√©rentes r√©solutions pour am√©liorer la d√©tection:

```
Scale 1 (13√ó13) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Pr√©diction 1 (Large objects)
     ‚Üì Upsample √ó2
     + Concat Route 2
     ‚Üì
Scale 2 (26√ó26) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Pr√©diction 2 (Medium objects)
     ‚Üì Upsample √ó2
     + Concat Route 1
     ‚Üì
Scale 3 (52√ó52) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Pr√©diction 3 (Small objects)
```

### 3. Head: Multi-Scale Detection

Trois t√™tes de d√©tection ind√©pendantes:

| Scale | Grid Size | Anchors | Output Shape | D√©tecte |
|-------|-----------|---------|--------------|---------|
| Scale 1 | 13√ó13 | (116,90), (156,198), (373,326) | 13√ó13√ó255 | Grands objets |
| Scale 2 | 26√ó26 | (30,61), (62,45), (59,119) | 26√ó26√ó255 | Objets moyens |
| Scale 3 | 52√ó52 | (10,13), (16,30), (33,23) | 52√ó52√ó255 | Petits objets |

**Output par √©chelle:** 3 anchors √ó (20 classes + 5 params) = 255 channels
- 5 params: (x, y, w, h, confidence)

**Param√®tres totaux:** ~62 millions

## Dataset

### PASCAL VOC

Identique √† YOLOv1, le mod√®le utilise PASCAL VOC avec **20 classes:**

```
aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow,
diningtable, dog, horse, motorbike, person, pottedplant, sheep,
sofa, train, tvmonitor
```

**Sp√©cifications:**
- Images redimensionn√©es: 416√ó416 pixels (vs 448√ó448 pour YOLOv1)
- Format des labels: YOLO (x_center, y_center, width, height)
- Augmentation de donn√©es: rotation, flip, color jitter, etc.

### T√©l√©chargement

```bash
# Via Kaggle API
kaggle datasets download -d aladdinpersson/pascal-voc-yolo

# Extraction dans le dossier data/
unzip pascal-voc-yolo.zip -d data/PASCAL_VOC/
```

**Structure attendue:**
```
data/PASCAL_VOC/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îú‚îÄ‚îÄ labels/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îú‚îÄ‚îÄ train.csv
‚îî‚îÄ‚îÄ val.csv
```

## Fonction de perte

La loss YOLOv3 est calcul√©e pour chaque √©chelle et combin√©e:

```python
Total Loss = Œ£ (Box Loss + Object Loss + No-Object Loss + Class Loss)
             scale ‚àà {1,2,3}
```

### Composantes de la loss

1. **Box Loss (Localization):**
   - MSE sur les coordonn√©es (x, y)
   - MSE sur les dimensions (w, h)
   - Appliqu√© uniquement aux cellules contenant des objets

2. **Object Loss (Confidence):**
   - Binary Cross-Entropy sur le score de confiance
   - Pour les cellules contenant des objets

3. **No-Object Loss:**
   - Binary Cross-Entropy sur le score de confiance
   - Pour les cellules sans objet
   - Poids r√©duit (g√©n√©ralement 0.5)

4. **Class Loss:**
   - Binary Cross-Entropy sur les probabilit√©s de classe
   - Multi-label (un objet peut appartenir √† plusieurs classes)

### Caract√©ristiques

- **IoU-based assignment:** Chaque anchor est assign√© √† l'objet avec le meilleur IoU
- **Ignore threshold:** Anchors avec IoU > 0.5 sont ignor√©s (ni obj ni no-obj)
- **Multi-label classification:** Contrairement √† YOLOv1 (single-label)

## Installation et utilisation

### Pr√©requis

```bash
pip install -r requirements.txt
```

**D√©pendances principales:**
```
torch>=2.0.0
torchvision>=0.15.0
numpy
pandas
Pillow
tqdm
matplotlib
albumentations  # Pour l'augmentation de donn√©es
opencv-python
```

### Configuration

Le fichier `config.py` contient tous les hyperparam√®tres:

```python
# Dataset
DATASET = 'data/PASCAL_VOC'
IMAGE_SIZE = 416
NUM_CLASSES = 20

# Training
BATCH_SIZE = 32
LEARNING_RATE = 1e-5
WEIGHT_DECAY = 1e-4
NUM_EPOCHS = 100

# Detection
CONF_THRESHOLD = 0.05     # Seuil de confiance
NMS_IOU_THRESH = 0.45     # Seuil NMS
MAP_IOU_THRESH = 0.5      # Seuil mAP

# Anchors (width, height) normalis√©s
ANCHORS = [
    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],    # Scale 1
    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],   # Scale 2
    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],   # Scale 3
]
```

### Entra√Ænement

```bash
# Entra√Ænement depuis z√©ro
python train.py

# Reprendre depuis un checkpoint
python train.py --load_model --checkpoint checkpoints/checkpoint.pth.tar
```

### Structure des fichiers

```
yolov3_from_scratch/
‚îú‚îÄ‚îÄ model.py          # Architecture YOLOv3 (Darknet-53 + FPN)
‚îú‚îÄ‚îÄ loss.py           # Fonction de perte multi-√©chelle
‚îú‚îÄ‚îÄ dataset.py        # Chargement des donn√©es avec anchors
‚îú‚îÄ‚îÄ train.py          # Script d'entra√Ænement
‚îú‚îÄ‚îÄ utils.py          # Utilitaires (IoU, NMS, mAP, visualisation)
‚îú‚îÄ‚îÄ config.py         # Configuration et hyperparam√®tres
‚îú‚îÄ‚îÄ requirements.txt  # D√©pendances Python
‚îî‚îÄ‚îÄ README.md         # Cette documentation
```

## üìà Entra√Ænement et performance

### Sp√©cifications d'entra√Ænement

- **Dur√©e:** ~8-10 heures sur GPU A100
- **Dataset:** PASCAL VOC 2007+2012
- **Batch size:** 32
- **Learning rate:** 1e-5 avec weight decay 1e-4
- **Optimizer:** Adam
- **Augmentation:** Rotation, flip, color jitter, affine transforms

### M√©triques

- **mAP@0.5:** Mean Average Precision avec IoU threshold = 0.5
- **mAP@0.5:0.95:** mAP moyenn√© sur plusieurs seuils IoU
- **FPS:** Frames Per Second pour l'inf√©rence

### Comparaison YOLOv1 vs YOLOv3

| M√©trique | YOLOv1 | YOLOv3 | Am√©lioration |
|----------|--------|--------|--------------|
| mAP@0.5 | ~63% | ~74% | +11% |
| Petits objets | Faible | Bon | ‚≠ê‚≠ê‚≠ê |
| FPS (GPU) | ~45 | ~30 | -33% |
| Param√®tres | 45M | 62M | +38% |
| Grid cells | 7√ó7 | 13√ó13 + 26√ó26 + 52√ó52 | Multi-√©chelle |

## üîç Fonctions utilitaires

### Intersection over Union (IoU)

```python
# IoU pour width/height (utilis√© pour anchor matching)
iou_width_height(boxes1, boxes2)

# IoU complet avec coordonn√©es
intersection_over_union(boxes_preds, boxes_labels, box_format="midpoint")
```

### Non-Maximum Suppression (NMS)

```python
non_max_suppression(
    predictions, 
    iou_threshold=0.45, 
    threshold=0.4, 
    box_format="midpoint"
)
```

### Mean Average Precision (mAP)

```python
mean_average_precision(
    pred_boxes, 
    true_boxes, 
    iou_threshold=0.5, 
    num_classes=20
)
```

### Conversion et visualisation

```python
# Convertir les pr√©dictions de grille en bounding boxes
cells_to_bboxes(predictions, anchors, S, is_preds=True)

# Visualiser les r√©sultats
plot_image(image, boxes)
plot_couple_examples(model, loader, threshold=0.6)

# Sauvegarder/charger le mod√®le
save_checkpoint(model, optimizer, filename="checkpoint.pth.tar")
load_checkpoint(checkpoint_file, model, optimizer, lr)
```

## üìö R√©f√©rences acad√©miques

**Article original YOLOv3:**
```
Redmon, J., & Farhadi, A. (2018).
"YOLOv3: An Incremental Improvement."
arXiv preprint arXiv:1804.02767.
```

**Lien:** [arXiv:1804.02767](https://arxiv.org/abs/1804.02767)

**Articles connexes:**
- YOLOv1: [arXiv:1506.02640](https://arxiv.org/abs/1506.02640)
- YOLOv2/YOLO9000: [arXiv:1612.08242](https://arxiv.org/abs/1612.08242)
- Feature Pyramid Networks: [arXiv:1612.03144](https://arxiv.org/abs/1612.03144)
- ResNet: [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

## D√©tails d'impl√©mentation

### Residual Blocks

```python
class ResidualBlock(nn.Module):
    """Bloc r√©siduel avec connexion de saut"""
    def __init__(self, channels, num_repeats=1):
        super().__init__()
        self.layers = nn.ModuleList()
        for _ in range(num_repeats):
            self.layers.append(
                nn.Sequential(
                    CNN(channels, channels // 2, kernel_size=1),      # R√©duction
                    CNN(channels // 2, channels, kernel_size=3, padding=1),  # Expansion
                )
            )
    
    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)  # Connexion r√©siduelle
        return x
```

### Scale Prediction

```python
class ScalePrediction(nn.Module):
    """Pr√©diction d'√©chelle pour la sortie YOLO"""
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.pred = nn.Sequential(
            CNN(in_channels, 2 * in_channels, kernel_size=3, padding=1),
            CNN(2 * in_channels, (num_classes + 5) * 3, kernel_size=1, bn_act=False),
        )
    
    def forward(self, x):
        # Reshape: [batch, 3, grid, grid, num_classes + 5]
        return (
            self.pred(x)
            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])
            .permute(0, 1, 3, 4, 2)
        )
```

### Augmentation de donn√©es

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2

train_transforms = A.Compose([
    A.LongestMaxSize(max_size=int(IMAGE_SIZE * 1.1)),
    A.PadIfNeeded(min_height=int(IMAGE_SIZE * 1.1), 
                  min_width=int(IMAGE_SIZE * 1.1)),
    A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),
    A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),
    A.Affine(shear=15, rotate=20, p=0.5),
    A.HorizontalFlip(p=0.5),
    A.Blur(p=0.1),
    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),
    ToTensorV2(),
], bbox_params=A.BboxParams(format="yolo", min_visibility=0.4, label_fields=[]))
```

## Avantages et am√©liorations

### Avantages de YOLOv3

1. **D√©tection multi-√©chelle:**
   - Excellente performance sur petits, moyens et grands objets
   - Grilles de 52√ó52, 26√ó26 et 13√ó13 cellules

2. **Architecture r√©siduelle:**
   - Meilleure propagation du gradient
   - Entra√Ænement plus stable et profond (53 couches)

3. **Feature Pyramid Network:**
   - Combine features de haute et basse r√©solution
   - Enrichit la repr√©sentation des features

4. **Classification multi-label:**
   - Un objet peut appartenir √† plusieurs classes
   - Plus flexible que YOLOv1

5. **Anchors optimis√©s:**
   - 9 anchors adapt√©s aux diff√©rentes √©chelles
   - Meilleure couverture des ratios d'aspect

### Am√©liorations possibles

- **Architecture plus r√©cente:** YOLOv4, YOLOv5, YOLOX
- **Attention mechanisms:** CBAM, SE-Net
- **Data augmentation avanc√©e:** Mosaic, MixUp
- **Loss functions:** GIoU, DIoU, CIoU
- **Post-processing:** Soft-NMS, DIoU-NMS

## üìùNotes importantes

‚ö†Ô∏è **Checkpoints et mod√®les:**
- Les poids pr√©-entra√Æn√©s ne sont pas inclus (fichiers volumineux)
- Le fichier `checkpoints/checkpoint.pth.tar` doit √™tre t√©l√©charg√© s√©par√©ment
- Dur√©e d'entra√Ænement : ~8-10h sur GPU A100

‚ö†Ô∏è **Dataset:**
- Le dataset PASCAL VOC doit √™tre t√©l√©charg√© via Kaggle
- Environ 20GB d'espace disque n√©cessaire
- Structure de dossiers sp√©cifique requise (voir section Dataset)

‚ö†Ô∏è **Ressources:**
- GPU recommand√© (training impossible sur CPU)
- Minimum 16GB de RAM
- ~25GB d'espace disque total (dataset + checkpoints)
